# -*- coding: utf-8 -*-
# Generated by Django 1.10.6 on 2017-03-29 15:14
from __future__ import unicode_literals

from django.db import migrations, models


class Migration(migrations.Migration):

    initial = True

    dependencies = [
    ]

    operations = [
        migrations.CreateModel(
            name='Photo',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('image', models.ImageField(upload_to='')),
                ('filtered_image', models.ImageField(upload_to='')),
                ('content', models.TextField(max_length=500)),
                ('created_at', models.DateTimeField(auto_now_add=True)),
            ],
        ),
    ]

# This file is part of the Hotwire Shell project API.

# Copyright (C) 2007 Colin Walters <walters@verbum.org>

# Permission is hereby granted, free of charge, to any person obtaining a copy 
# of this software and associated documentation files (the "Software"), to deal 
# in the Software without restriction, including without limitation the rights 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies 
# of the Software, and to permit persons to whom the Software is furnished to do so, 
# subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all 
# copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A 
# PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE X CONSORTIUM BE 
# LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, 
# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR 
# THE USE OR OTHER DEALINGS IN THE SOFTWARE.

import os,sys

from hotwire.cmdalias import AliasRegistry

default_aliases = {'sudo': 'term sudo',
                   'su': 'term su',                   
                   'vi': 'term vi',
                   'vim': 'term vim',
                   'gdb': 'term gdb',                   
                   'ssh': 'term ssh',
                   'man': 'term man',
                   'info': 'term info',
                   'most': 'term most',                   
                   'less': 'term less',
                   'more': 'term more',
                   'ipython': 'term ipython',                     
                   'top': 'term top',
                   'iotop': 'term iotop',
                   'htop': 'term htop',                     
                   'powertop': 'term powertop',                   
                   'nano': 'term nano',
                   'pico': 'term pico',
                   'irssi': 'term irssi',
                   'mutt': 'term mutt',
                  }
aliases = AliasRegistry.getInstance()
for name,value in default_aliases.iteritems():
    aliases.insert(name, value)

# -*- coding: utf-8 -*-
# Generated by Django 1.10.6 on 2017-03-30 14:05
from __future__ import unicode_literals

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('photos', '0001_initial'),
    ]

    operations = [
        migrations.AlterField(
            model_name='photo',
            name='content',
            field=models.TextField(blank=True, max_length=500, null=True),
        ),
    ]

# -*- coding: utf-8 -*-
# Generated by Django 1.10.6 on 2017-04-05 12:24
from __future__ import unicode_literals

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('photos', '0002_auto_20170330_2305'),
    ]

    operations = [
        migrations.AddField(
            model_name='photo',
            name='user',
            field=models.ForeignKey(default=1, on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL),
            preserve_default=False,
        ),
        migrations.AlterField(
            model_name='photo',
            name='filtered_image',
            field=models.ImageField(upload_to='%Y/%m/%d/filtered'),
        ),
        migrations.AlterField(
            model_name='photo',
            name='image',
            field=models.ImageField(upload_to='%Y/%m/%d/orig'),
        ),
    ]

# This file is part of the Hotwire Shell user interface.
#   
# Copyright (C) 2007 Colin Walters <walters@verbum.org>
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

import os, sys, logging, StringIO, traceback

import cairo, gtk, gobject, pango

from hotwire.sysdep.fs import Filesystem
from hotwire.logutil import log_except
import hotwire.version

_logger = logging.getLogger("hotwire.AboutDialog")

class HotwireAboutDialog(gtk.AboutDialog):
    def __init__(self):
        super(HotwireAboutDialog, self).__init__()
        dialog = self
        dialog.set_property('website', 'http://hotwire-shell.org')
        dialog.set_property('version', hotwire.version.__version__)
        dialog.set_property('authors', ['Colin Walters <walters@verbum.org>'])
        dialog.set_property('copyright', u'Copyright \u00A9 2007,2008 Colin Walters <walters@verbum.org>')
        dialog.set_property('logo-icon-name', 'hotwire')
        dialog.set_property('license', 
                            '''Hotwire is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.\n
Hotwire is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.\n
You should have received a copy of the GNU General Public License
along with Hotwire; if not, write to the Free Software Foundation, Inc.,
51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA''')
        dialog.set_property('name', "Hotwire")
        comments = _("An object-oriented hypershell\n\n")
        if hotwire.version.svn_version_info:
            comments += "changeset: %s\ndate: %s\n" % (hotwire.version.svn_version_info['Revision'], hotwire.version.svn_version_info['Last Changed Date'],)
        dialog.set_property('comments', comments)

from django.contrib import admin

# Register your models here.

#!/usr/bin/env python

'''
analyze-schema-compression.py

* Copyright 2014, Amazon.com, Inc. or its affiliates. All Rights Reserved.
*
* Licensed under the Amazon Software License (the "License").
* You may not use this file except in compliance with the License.
* A copy of the License is located at
*
* http://aws.amazon.com/asl/
*
* or in the "license" file accompanying this file. This file is distributed
* on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
* express or implied. See the License for the specific language governing
* permissions and limitations under the License.

Analyses all tables in a Redshift Cluster Schema, and outputs a SQL script to 
migrate database tables with sub-optimal column encodings to optimal column
encodings as recommended by the database engine.

The processing model that the script will generate is:
    create new table XXX_$mig
    insert select * from old table into new table
    analyze new table
    rename old table to XXX_$old or drop table
    rename new table to old table

Use with caution on a running system


Ian Meyers
Amazon Web Services (2014)
'''

import sys
import os
from multiprocessing import Pool
import getopt
import re
import getpass
import time
import traceback
import pg8000
import shortuuid
import datetime
from _curses import OK
import math

__version__ = ".9.2.5"

OK = 0
ERROR = 1
INVALID_ARGS = 2
NO_WORK = 3
TERMINATED_BY_USER = 4
NO_CONNECTION = 5

# timeout for retries - 100ms
RETRY_TIMEOUT = 100. / 1000
    
# compiled regular expressions
IDENTITY_RE = re.compile(r'"identity"\((?P<current>.*), (?P<base>.*), \(?\'(?P<seed>\d+),(?P<step>\d+)\'.*\)')

def get_env_var(name, defaultVal):
    return os.environ[name] if name in os.environ else defaultVal

master_conn = None
db_connections = {}
db = get_env_var('PGDATABASE', None)
db_user = get_env_var('PGUSER', None)
db_pwd = None
db_host = get_env_var('PGHOST', None)
db_port = get_env_var('PGPORT', 5439)
analyze_schema = 'public'
target_schema = None
analyze_table = None
debug = False
threads = 2
output_file = None
output_file_handle = None
do_execute = False
query_slot_count = 1
ignore_errors = False
force = False
drop_old_data = False
comprows = None
query_group = None
ssl_option = False


def execute_query(str):
    conn = get_pg_conn()
    cursor = conn.cursor()
    cursor.execute(str)
    
    try:
        results = cursor.fetchall()
    except pg8000.ProgrammingError as e:
        if "no result set" in str(e):
            return None
        else:
            raise e
        
    return results
    
    
def close_conn(conn):
    try:
        conn.close()
    except Exception as e:
        if debug:
            print(e)
         
            
def cleanup():
    # close all connections and close the output file
    if master_conn != None:
        close_conn(master_conn)
    
    for key in db_connections:
        if db_connections[key] != None:            
            close_conn(db_connections[key]) 
    
    if output_file_handle != None:
        output_file_handle.close()


def comment(string):
    if (string != None):
        if re.match('.*\\n.*', string) != None:
            write('/* [%s]\n%s\n*/\n' % (str(os.getpid()), string))
        else:
            write('-- [%s] %s' % (str(os.getpid()), string))


def print_statements(statements):
    if statements != None:
        for s in statements:
            if s != None:
                write(s)
        
        
def write(s):
    # write output to all the places we want it
    print(s)
    if output_file_handle != None:
        output_file_handle.write(str(s) + "\n")
        output_file_handle.flush()
        
        
def get_pg_conn():
    global db_connections
    pid = str(os.getpid())
    
    conn = None
    
    # get the database connection for this PID
    try:
        conn = db_connections[pid]
    except KeyError:
        pass
        
    if conn == None:
        # connect to the database
        if debug:
            comment('Connect [%s] %s:%s:%s:%s' % (pid, db_host, db_port, db, db_user))
        
        try:
            conn = pg8000.connect(user=db_user, host=db_host, port=db_port, database=db, password=db_pwd, ssl=ssl_option, timeout=None, keepalives=1, keepalives_idle=200, keepalives_interval=200, keepalives_count=5)
        except Exception as e:
            write(e)
            write('Unable to connect to Cluster Endpoint')
            cleanup()
            return ERROR      
        
        # set default search path        
        search_path = 'set search_path = \'$user\',public,%s' % (analyze_schema)
        if target_schema != None and target_schema != analyze_schema:
            search_path = search_path + ', %s' % (target_schema)
            
        if debug:
            comment(search_path)
        
        cursor = None
        try:
            cursor = conn.cursor()
            cursor.execute(search_path)
        except pg8000.Error as e:
            if re.match('schema "%s" does not exist' % (analyze_schema,), e.message) != None:
                write('Schema %s does not exist' % (analyze_schema,))
            else:
                write(e.message)
            return None

        if query_group != None:
            set_query_group = 'set query_group to %s' % (query_group)

            if debug:
                comment(set_query_group)

            cursor.execute(set_query_group)
        
        if query_slot_count != None and query_slot_count != 1:
            set_slot_count = 'set wlm_query_slot_count = %s' % (query_slot_count)
            
            if debug:
                comment(set_slot_count)
                
            cursor.execute(set_slot_count)

        # set a long statement timeout
        set_timeout = "set statement_timeout = '1200000'"
        if debug:
            comment(set_timeout)
            
        cursor.execute(set_timeout)
        
        # cache the connection
        db_connections[pid] = conn
        
    return conn


def get_identity(adsrc):
    # checks if a column defined by adsrc (column from pg_attrdef) is
    # an identity, since both identities and defaults end up in this table
    # if is identity returns (seed, step); if not returns None
    # TODO there ought be a better way than using a regex
    m = IDENTITY_RE.match(adsrc)
    if m:
        return m.group('seed'), m.group('step')
    else:
        return None


def get_foreign_keys(analyze_schema, target_schema, table_name):
    has_fks = False
    
    fk_statement = '''SELECT /* fetching foreign key relations */ conname,
  pg_catalog.pg_get_constraintdef(cons.oid, true) as condef
 FROM pg_catalog.pg_constraint cons,
 pg_namespace pgn,
 pg_class pgc
 WHERE cons.conrelid = pgc.oid
 and pgn.nspname = '%s'
 and pgc.relnamespace = pgn.oid
 and pgc.oid = '%s'::regclass
 AND cons.contype = 'f'
 ORDER BY 1
''' % (analyze_schema, table_name)

    if (debug):
        comment(fk_statement)
    
    foreign_keys = execute_query(fk_statement)
    fk_statements = []
    
    for fk in foreign_keys:
        has_fks = True
        references_clause = fk[1].replace('REFERENCES ', 'REFERENCES %s.' % (target_schema))      
        fk_statements.append('alter table %s."%s" add constraint %s %s;' % (target_schema, table_name, fk[0], references_clause))
    
    if has_fks:
        return fk_statements
    else:
        return None
            
            
def get_primary_key(table_schema, target_schema, original_table, new_table):
    pk_statement = 'alter table %s."%s" add primary key (' % (target_schema, new_table)
    has_pks = False
    
    # get the primary key columns
    statement = '''SELECT /* fetch primary key information */   
  att.attname
FROM pg_index ind, pg_class cl, pg_attribute att, pg_namespace pgn
WHERE 
  cl.oid = '%s'::regclass 
  AND ind.indrelid = cl.oid 
  AND att.attrelid = cl.oid
  and cl.relnamespace = pgn.oid
  and pgn.nspname = '%s'
  and (ind.indkey[0] = att.attnum or 
       ind.indkey[1] = att.attnum or
       ind.indkey[2] = att.attnum or
       ind.indkey[3] = att.attnum or
       ind.indkey[4] = att.attnum
      )
  and attnum > 0
  AND ind.indisprimary
order by att.attnum;
''' % (original_table, table_schema)

    if debug:
        comment(statement)
            
    pks = execute_query(statement)
    
    for pk in pks:
        has_pks = True
        pk_statement = pk_statement + pk[0] + ','
        
    pk_statement = pk_statement[:-1] + ');'
    
    if has_pks:
        return pk_statement
    else:
        return None
    
        
def get_table_desc(table_name):
    # get the table definition from the dictionary so that we can get relevant details for each column
    statement = '''select /* fetching column descriptions for table */ "column", type, encoding, distkey, sortkey, "notnull", ad.adsrc
 from pg_table_def de, pg_attribute at LEFT JOIN pg_attrdef ad ON (at.attrelid, at.attnum) = (ad.adrelid, ad.adnum)
 where de.schemaname = '%s'
 and de.tablename = '%s'
 and at.attrelid = '%s."%s"'::regclass
 and de.column = at.attname
''' % (analyze_schema, table_name, analyze_schema, table_name)

    if debug:
        comment(statement)
        
    description = execute_query(statement)
    
    descr = {}
    for row in description:
        if debug:
            comment("Table Description: %s" % str(row))
        descr[row[0]] = row
        
    return descr


def get_count_raw_columns(table_name):
    # count the number of raw encoded columns which are not the sortkey, from the dictionary
    statement = '''select /* getting count of raw columns in table */ count(9) count_raw_columns
      from pg_table_def 
      where schemaname = '%s'
        and lower(encoding) in ('raw','none') 
        and sortkey != 1        
        and tablename = '%s'
''' % (analyze_schema, table_name)

    if debug:
        comment(statement)
        
    description = execute_query(statement)
    
    return description


def run_commands(conn, commands):
    for c in commands:
        if c != None:
            cursor = conn.cursor()
            comment('[%s] Running %s' % (str(os.getpid()), c))
            try:
                if c.count(';') > 1:
                    subcommands = c.split(';')
                    
                    for s in subcommands:
                        if s != None and s != '':
                            cursor.execute(s.replace("\n", ""))
                else:
                    cursor.execute(c)
                comment('Success.')
            except Exception as e:
                # cowardly bail on errors
                conn.rollback()
                write(traceback.format_exc())
                return False
    
    return True
        
        
def analyze(table_info):     
    table_name = table_info[0]
    dist_style = table_info[3]
    
    # get the count of columns that have raw encoding applied
    table_unoptimised = False
    count_unoptimised = 0
    encodings_modified = False    
    output = get_count_raw_columns(table_name)
    
    if output == None:
        write("Unable to determine potential RAW column encoding for %s" % table_name)
        return ERROR
    else:
        for row in output:
            if row[0] > 0:
                table_unoptimised = True
                count_unoptimised += row[0]
                
    if not table_unoptimised and not force:
        comment("Table %s does not require encoding optimisation" % table_name)
        return OK
    else:
        comment("Table %s contains %s unoptimised columns" % (table_name, count_unoptimised))
        if force:
            comment("Using Force Override Option")
    
        statement = 'analyze compression %s."%s"' % (analyze_schema, table_name)
        
        if comprows != None:
            statement = statement + (" comprows %s" % int(comprows))
            
        try:
            if debug:
                comment(statement)
                
            comment("Analyzing Table '%s'" % (table_name,))
        
            # run the analyze in a loop, because it could be locked by another process modifying rows and get a timeout
            analyze_compression_result = None
            analyze_retry = 10
            attempt_count = 0
            last_exception = None
            while attempt_count < analyze_retry and analyze_compression_result == None:
                try:
                    analyze_compression_result = execute_query(statement)
                except KeyboardInterrupt:
                    # To handle Ctrl-C from user
                    cleanup()
                    return TERMINATED_BY_USER
                except Exception as e:
                    write(e)
                    attempt_count += 1
                    last_exception = e
                    
                    # Exponential Backoff
                    time.sleep(2 ** attempt_count * RETRY_TIMEOUT)
    
            if analyze_compression_result == None:
                if last_exception != None:
                    write("Unable to analyze %s due to Exception %s" % (table_name, last_exception.message))
                else:
                    write("Unknown Error")
                return ERROR
            
            if target_schema == analyze_schema:
                target_table = '%s_$mig' % table_name
            else:
                target_table = table_name
            
            create_table = 'begin;\nlock table %s."%s";\ncreate table %s."%s"(' % (analyze_schema, table_name, target_schema, target_table,)
            
            # query the table column definition
            descr = get_table_desc(table_name)
                
            encode_columns = []
            statements = []
            sortkeys = {}
            has_zindex_sortkeys = False
            has_identity = False
            non_identity_columns = []
            fks = []
            
            # process each item given back by the analyze request
            for row in analyze_compression_result:
                if debug:
                    comment("Analyzed Compression Row State: %s" % str(row))
                col = row[1]
                
                # compare the previous encoding to the new encoding
                new_encoding = row[2]
                old_encoding = descr[col][2]
                old_encoding = 'raw' if old_encoding == 'none' else old_encoding
                if new_encoding != old_encoding:
                    encodings_modified = True
                    if debug:
                        comment("Column %s will be modified from %s encoding to %s encoding" % (col, old_encoding, new_encoding))
                
                # fix datatypesj from the description type to the create type
                col_type = descr[col][1]
                
                # check whether varchars columns are too wide
                if analyze_col_width and "character varying" in col_type:                 
                    curr_col_length = int(re.search(r'\d+', col_type).group())
                    if curr_col_length > 255:
                        col_len_statement = 'select /* computing max column length */ max(len(%s)) from %s."%s"' % (descr[col][0], analyze_schema, table_name)
                        try:
                            if debug:
                                comment(col_len_statement)
                
                            comment("Analyzing max length of character column '%s' for table '%s.%s' " % (col, analyze_schema, table_name))
        
                            # run the analyze in a loop, because it could be locked by another process modifying rows and get a timeout
                            col_len_result = None
                            col_len_retry = 10
                            col_len_attempt_count = 0
                            col_len_last_exception = None
                            while col_len_attempt_count < col_len_retry and col_len_result == None:
                                try:
                                    col_len_result = execute_query(col_len_statement)
                                except KeyboardInterrupt:
                                    # To handle Ctrl-C from user
                                    cleanup()
                                    return TERMINATED_BY_USER
                                except Exception as e:
                                    write(e)
                                    col_len_attempt_count += 1
                                    col_len_last_exception = e
                    
                                    # Exponential Backoff
                                    time.sleep(2 ** col_len_attempt_count * RETRY_TIMEOUT)
                                    
                            if col_len_result == None:
                                if col_len_last_exception != None:
                                    write("Unable to determine length of %s for table %s due to Exception %s" % (col, table_name, last_exception.message))
                                else:
                                    write("Unknown Error")
                                return ERROR

                            if debug:
                                comment("Max width of character column '%s' for table '%s.%s' is %d. Current width is %d." % (descr[col][0],  analyze_schema, table_name, col_len_result[0][0], curr_col_length))
        
                            if col_len_result[0][0] < curr_col_length: 
                                col_type = re.sub(str(curr_col_length), str(col_len_result[0][0]), col_type)
                                encodings_modified = True 
                    
                        except Exception as e:
                            write('Exception %s during analysis of %s' % (e.message, table_name))
                            write(traceback.format_exc())
                            return ERROR
                            
                col_type = col_type.replace('character varying', 'varchar').replace('without time zone', '')    
                
                # check whether number columns are too wide
                if analyze_col_width and "int" in col_type:  
                    col_len_statement = 'select max(%s) from %s."%s"' % (descr[col][0], analyze_schema, table_name)
                    try:
                        if debug:
                            comment(col_len_statement)
                
                        comment("Analyzing max column '%s' for table '%s.%s' " % (col, analyze_schema, table_name))
        
                        # run the analyze in a loop, because it could be locked by another process modifying rows and get a timeout
                        col_len_result = None
                        col_len_retry = 10
                        col_len_attempt_count = 0
                        col_len_last_exception = None
                        while col_len_attempt_count < col_len_retry and col_len_result == None:
                            try:
                                col_len_result = execute_query(col_len_statement)
                            except KeyboardInterrupt:
                                # To handle Ctrl-C from user
                                cleanup()
                                return TERMINATED_BY_USER
                            except Exception as e:
                                write(e)
                                col_len_attempt_count += 1
                                col_len_last_exception = e
                    
                                # Exponential Backoff
                                time.sleep(2 ** col_len_attempt_count * RETRY_TIMEOUT)
                                    
                        if col_len_result == None:
                            if col_len_last_exception != None:
                                write("Unable to determine length of %s for table %s due to Exception %s" % (col, table_name, last_exception.message))
                            else:
                                write("Unknown Error")
                            return ERROR

                        if debug:
                            comment("Max of column '%s' for table '%s.%s' is %d. Current column type is %s." % (descr[col][0],  analyze_schema, table_name, col_len_result[0][0], col_type))
                        
                        # Test to see if largest value is smaller than largest value of smallint (2 bytes)
                        if col_len_result[0][0] <= int(math.pow(2, 15)-1) and col_type != "smallint": 
                            col_type = re.sub(col_type, "smallint", col_type)
                            encodings_modified = True 
                        
                        # Test to see if largest value is smaller than largest value of smallint (4 bytes)    
                        elif col_len_result[0][0] <= int(math.pow(2, 31)-1) and col_type != "integer":
                            col_type = re.sub(col_type, "integer", col_type)
                            encodings_modified = True 

                    except Exception as e:
                        write('Exception %s during analysis of %s' % (e.message, table_name))
                        write(traceback.format_exc())
                        return ERROR    
                
                            
                # is this the dist key?
                distkey = descr[col][3]
                if str(distkey).upper()[0] == 'T':
                    distkey = 'DISTKEY'
                else:
                    distkey = ''
                    
                # is this the sort key?
                sortkey = descr[col][4]
                if sortkey != 0:
                    # add the absolute ordering of the sortkey to the list of all sortkeys
                    sortkeys[abs(sortkey)] = col
                    
                    if (sortkey < 0):
                        has_zindex_sortkeys = True
                    
                # don't compress first sort key
                if abs(sortkey) == 1:
                    compression = 'RAW'
                else:
                    compression = row[2]
                    
                # extract null/not null setting            
                col_null = descr[col][5]
                
                if str(col_null).upper() == 'TRUE':
                    col_null = 'NOT NULL'
                else:
                    col_null = ''
    
                # get default or identity syntax for this column
                default_or_identity = descr[col][6]
                if default_or_identity:
                    ident_data = get_identity(default_or_identity)
                    if ident_data is None:
                        default_value = 'default %s' % default_or_identity
                        non_identity_columns.append(col)
                    else:
                        default_value = 'identity (%s, %s)' % ident_data
                        has_identity = True
                else:
                    default_value = ''
                    non_identity_columns.append(col)
    
                # add the formatted column specification
                encode_columns.extend(['"%s" %s %s %s encode_sentence %s %s'
                                       % (col, col_type, default_value, col_null, compression, distkey)])
            
            # if this table's encodings have not changed, then don't do a modification, unless force options is set
            if (not force) and (not encodings_modified):
                comment("Column Encoding resulted in an identical table - no changes will be made")
            else:
                comment("Column Encoding will be modified for %s.%s" % (analyze_schema, table_name))
                
                # add all the column encoding statements on to the create table statement, suppressing the leading comma on the first one
                for i, s in enumerate(encode_columns):
                    create_table += '\n%s%s' % ('' if i == 0 else ',', s)
        
                create_table = create_table + '\n)\n'
                
                # add diststyle all if needed
                if dist_style == 'ALL':
                    create_table = create_table + 'diststyle all\n'
                    
                # add sort key as a table block to accommodate multiple columns
                if len(sortkeys) > 0:
                    sortkey = '%sSORTKEY(' % ('INTERLEAVED ' if has_zindex_sortkeys else '')    
                    
                    for i in range(1, len(sortkeys) + 1):
                        sortkey = sortkey + sortkeys[i]
                       
                        if i != len(sortkeys):
                            sortkey = sortkey + ','
                        else:
                            sortkey = sortkey + ')\n'
                    create_table = create_table + (' %s ' % sortkey)                
                
                create_table = create_table + ';'
                
                # run the create table statement
                statements.extend([create_table])         
                
                # get the primary key statement
                statements.extend([get_primary_key(analyze_schema, target_schema, table_name, target_table)]);
    
                # insert the old data into the new table
                # if we have identity column(s), we can't insert data from them, so do selective insert
                if has_identity:
                    source_columns = ', '.join(non_identity_columns)
                    mig_columns = '(' + source_columns + ')'
                else:
                    source_columns = '*'
                    mig_columns = ''
    
                insert = 'insert into %s."%s" %s select %s from %s."%s";' % (target_schema,
                                                                         target_table,
                                                                         mig_columns,
                                                                         source_columns,
                                                                         analyze_schema,
                                                                         table_name)
                statements.extend([insert])
                        
                # analyze the new table
                analyze = 'analyze %s."%s";' % (target_schema, target_table)
                statements.extend([analyze])
                        
                if (target_schema == analyze_schema):
                    # rename the old table to _$old or drop
                    if drop_old_data:
                        drop = 'drop table %s."%s" cascade;' % (target_schema, table_name)
                    else:
                        # the alter table statement for the current data will use the first 104 characters of the original table name, the current datetime as YYYYMMDD and a 10 digit random string
                        drop = 'alter table %s."%s" rename to "%s_%s_%s_$old";' % (target_schema, table_name, table_name[0:104] , datetime.date.today().strftime("%Y%m%d") , shortuuid.ShortUUID().random(length=10))
                    
                    statements.extend([drop])
                            
                    # rename the migrate table to the old table name
                    rename = 'alter table %s."%s" rename to "%s";' % (target_schema, target_table, table_name)
                    statements.extend([rename])
                
                # add foreign keys
                fks = get_foreign_keys(analyze_schema, target_schema, table_name)
                
                statements.extend(['commit;'])
                
                if do_execute:
                    if not run_commands(get_pg_conn(), statements):
                        if not ignore_errors:
                            if debug:
                                write("Error running statements: %s" % (str(statements),))
                            return ERROR
                else:
                    comment("No encoding modifications required for %s.%s" % (analyze_schema, table_name))    
        except Exception as e:
            write('Exception %s during analysis of %s' % (e.message, table_name))
            write(traceback.format_exc())
            return ERROR
        
        print_statements(statements)
        
        return (OK, fks, encodings_modified)


def usage(with_message):
    write('Usage: analyze-schema-compression.py')
    write('       Generates a script to optimise Redshift column encodings on all tables in a schema\n')
    
    if with_message != None:
        write(with_message + "\n")
        
    write('Arguments: --db             - The Database to Use')
    write('           --db-user        - The Database User to connect to')
    write('           --db-pwd         - The Password for the Database User to connect to')
    write('           --db-host        - The Cluster endpoint')
    write('           --db-port        - The Cluster endpoint port (default 5439)')
    write('           --analyze-schema - The Schema to be Analyzed (default public)')
    write('           --analyze-table  - A specific table to be Analyzed, if --analyze-schema is not desired')
    write('           --analyze-cols   - Analyze column width and reduce the column width if needed')
    write('           --target-schema  - Name of a Schema into which the newly optimised tables and data should be created, rather than in place')
    write('           --threads        - The number of concurrent connections to use during analysis (default 2)')
    write('           --output-file    - The full path to the output file to be generated')
    write('           --debug          - Generate Debug Output including SQL Statements being run')
    write('           --do-execute     - Run the compression encoding optimisation')
    write('           --slot-count     - Modify the wlm_query_slot_count from the default of 1')
    write('           --ignore-errors  - Ignore errors raised in threads when running and continue processing')
    write('           --force          - Force table migration even if the table already has Column Encoding applied')
    write('           --drop-old-data  - Drop the old version of the data table, rather than renaming')
    write('           --comprows       - Set the number of rows to use for Compression Encoding Analysis')
    write('           --query_group    - Set the query_group for all queries')
    write('           --ssl-option     - Set SSL to True or False (default False)')
    sys.exit(INVALID_ARGS)


# method used to configure global variables, so that we can call the run method
def configure(_output_file, _db, _db_user, _db_pwd, _db_host, _db_port, _analyze_schema, _target_schema, _analyze_table, _analyze_col_width, _threads, _do_execute, _query_slot_count, _ignore_errors, _force, _drop_old_data, _comprows, _query_group, _debug, _ssl_option):
    # setup globals
    global db
    global db_user
    global db_pwd
    global db_host
    global db_port
    global threads
    global analyze_schema
    global analyze_table
    global analyze_col_width
    global target_schema
    global debug
    global do_execute
    global query_slot_count
    global ignore_errors
    global force
    global drop_old_data
    global comprows
    global query_group
    global output_file
    global ssl_option

    # set global variable values
    output_file = _output_file    
    db = None if _db == "" else _db
    db_user = _db_user
    db_pwd = _db_pwd
    db_host = _db_host
    db_port = _db_port
    analyze_schema = None if _analyze_schema == "" else _analyze_schema
    analyze_table = None if _analyze_table == "" else _analyze_table
    target_schema = _analyze_schema if _target_schema == "" or _target_schema == None else _target_schema
    analyze_col_width = False if _analyze_col_width == None else _analyze_col_width
    debug = False if _debug == None else _debug    
    do_execute = False if _do_execute == None else _do_execute
    ignore_errors = False if _ignore_errors == None else _ignore_errors
    force = False if _force == None else _force
    drop_old_data = False if _drop_old_data == None else _drop_old_data
    query_group = None if _query_group == "" else _query_group
    threads = 1 if _threads == None else int(_threads)
    comprows = None if _comprows == -1 or _comprows == None else int(_comprows)
    query_slot_count = None if _query_slot_count == -1 or _query_slot_count == None else int(_query_slot_count)
    ssl_option = False if _ssl_option == None else _ssl_option
    
    if (debug == True):
        comment("Redshift Column Encoding Utility Configuration")
        comment("output_file: %s " % (output_file))
        comment("db: %s " % (db))
        comment("db_user: %s " % (db_user))
        comment("db_host: %s " % (db_host))
        comment("db_port: %s " % (db_port))
        comment("threads: %s " % (threads))
        comment("analyze_schema: %s " % (analyze_schema))
        comment("analyze_table: %s " % (analyze_table))
        comment("analyze_col: %s " % (analyze_col_width))
        comment("target_schema: %s " % (target_schema))
        comment("debug: %s " % (debug))
        comment("do_execute: %s " % (do_execute))
        comment("query_slot_count: %s " % (query_slot_count))
        comment("ignore_errors: %s " % (ignore_errors))
        comment("force: %s " % (force))
        comment("drop_old_data: %s " % (drop_old_data))
        comment("comprows: %s " % (comprows))
        comment("query_group: %s " % (query_group))
        comment("ssl_option: %s " % (ssl_option))
    
    
def run():
    global master_conn
    global output_file_handle
    
    # open the output file
    output_file_handle = open(output_file, 'w')
    
    # get a connection for the controlling processes
    master_conn = get_pg_conn()
    
    if master_conn == None or master_conn == ERROR:
        return NO_CONNECTION
    
    comment("Connected to %s:%s:%s as %s" % (db_host, db_port, db, db_user))
    if analyze_table != None:
        snippet = "Table '%s'" % analyze_table        
    else:
        snippet = "Schema '%s'" % analyze_schema
        
    comment("Analyzing %s for Columnar Encoding Optimisations with %s Threads..." % (snippet, threads))
    
    if do_execute:
        if drop_old_data:
            really_go = getpass.getpass("This will make irreversible changes to your database, and cannot be undone. Type 'Yes' to continue: ")
            
            if not really_go == 'Yes':
                write("Terminating on User Request")
                return TERMINATED_BY_USER

        comment("Recommended encoding changes will be applied automatically...")
    else:
        pass
    
    if analyze_table != None:        
        statement = '''select trim(a.name) as table, b.mbytes, a.rows, decode(pgc.reldiststyle,0,'EVEN',1,'KEY',8,'ALL') dist_style
from (select db_id, id, name, sum(rows) as rows from stv_tbl_perm a group by db_id, id, name) as a
join pg_class as pgc on pgc.oid = a.id
join pg_namespace as pgn on pgn.oid = pgc.relnamespace
join (select tbl, count(*) as mbytes
from stv_blocklist group by tbl) b on a.id=b.tbl
and pgn.nspname = '%s' and pgc.relname = '%s'        
        ''' % (analyze_schema, analyze_table)        
    else:
        # query for all tables in the schema ordered by size descending
        comment("Extracting Candidate Table List...")
        
        statement = '''select trim(a.name) as table, b.mbytes, a.rows, decode(pgc.reldiststyle,0,'EVEN',1,'KEY',8,'ALL') dist_style
from (select db_id, id, name, sum(rows) as rows from stv_tbl_perm a group by db_id, id, name) as a
join pg_class as pgc on pgc.oid = a.id
join pg_namespace as pgn on pgn.oid = pgc.relnamespace
join (select tbl, count(*) as mbytes
from stv_blocklist group by tbl) b on a.id=b.tbl
where pgn.nspname = '%s'
  and a.name::text SIMILAR TO '[A-Za-z0-9_]*'
order by 2;
        ''' % (analyze_schema,)
    
    if debug:
        comment(statement)
    
    query_result = execute_query(statement)
    
    if query_result == None:
        comment("Unable to issue table query - aborting")
        return ERROR
    
    analyze_tables = []
    for row in query_result:
        analyze_tables.append(row)
    
    comment("Analyzing %s table(s) which contain allocated data blocks" % (len(analyze_tables)))

    if debug:
        [comment(str(x)) for x in analyze_tables]

    result = []
    
    if analyze_tables != None:   
        # we'll use a Pool to process all the tables with multiple threads, or just sequentially if 1 thread is requested         
        if threads > 1:
            # setup executor pool
            p = Pool(threads)
        
            try:
                # run all concurrent steps and block on completion
                result = p.map(analyze, analyze_tables)
            except KeyboardInterrupt:
                # To handle Ctrl-C from user
                p.close()
                p.terminate()
                cleanup()
                return TERMINATED_BY_USER
            except:
                write(traceback.format_exc())
                p.close()
                p.terminate()
                cleanup()
                return ERROR
                
            p.terminate()
        else:
            for t in analyze_tables:
                result.append(analyze(t))
    else:
        comment("No Tables Found to Analyze")
    
    # return any non-zero worker output statuses
    modified_tables = 0
    for ret in result:
        if isinstance(ret, (list, tuple)):
            return_code = ret[0]
            fk_commands = ret[1]
            modified_tables = modified_tables + 1 if ret[2] else modified_tables
        else:
            return_code = ret
            fk_commands = None
        
        if fk_commands != None and len(fk_commands) > 0:
            print_statements(fk_commands)
            
            if do_execute:
                if not run_commands(master_conn, fk_commands):
                    if not ignore_errors:
                        write("Error running commands %s" % (fk_commands,))
                        return ERROR
            
        if return_code != OK:
            write("Error in worker thread: return code %d. Exiting." % (return_code,))
            return return_code
    
    comment("Performed modification of %s tables" % modified_tables)
    
    if (do_execute):
        if not master_conn.commit():
            return ERROR
    
    comment('Processing Complete')
    cleanup()    
    
    return OK


def main(argv):
    output_file = None
    db = None
    db_user = None
    db_pwd = None
    db_host = None
    db_port = None
    threads = None
    analyze_schema = None
    analyze_table = None
    analyze_col_width = None
    target_schema = None
    debug = None
    do_execute = None
    query_slot_count = None
    ignore_errors = None
    force = None
    drop_old_data = None
    comprows = None
    query_group = None
    ssl_option = None
    
    supported_args = """db= db-user= db-pwd= db-host= db-port= target-schema= analyze-schema= analyze-table= analyze-cols= threads= debug= output-file= do-execute= slot-count= ignore-errors= force= drop-old-data= comprows= query_group= ssl-option="""
    
    # extract the command line arguments
    try:
        optlist, remaining = getopt.getopt(argv[1:], "", supported_args.split())
    except getopt.GetoptError as err:
        print str(err)
        usage(None)

    # parse command line arguments
    for arg, value in optlist:
        if arg == "--db":
            if value == '' or value == None:
                usage()
            else:
                db = value
        elif arg == "--db-user":
            if value == '' or value == None:
                usage()
            else:
                db_user = value               
        elif arg == "--db-host":
            if value == '' or value == None:
                usage()
            else:
                db_host = value
        elif arg == "--db-port":
            if value != '' and value != None:
                db_port = int(value)
        elif arg == "--db-pwd":
            if value != '' and value != None:
                db_pwd = value
        elif arg == "--analyze-schema":
            if value != '' and value != None:
                analyze_schema = value
        elif arg == "--analyze-table":
            if value != '' and value != None:
                analyze_table = value
        elif arg == "--analyze-cols":
            if value != '' and value != None:
                analyze_col_width = value
                
        elif arg == "--target-schema":
            if value != '' and value != None:
                target_schema = value
        elif arg == "--threads":
            if value != '' and value != None:
                threads = int(value)
        elif arg == "--debug":
            if value == 'true' or value == 'True':
                debug = True
            else:
                debug = False
        elif arg == "--output-file":
            if value == '' or value == None:
                usage()
            else:
                output_file = value
        elif arg == "--ignore-errors":
            if value == 'true' or value == 'True':
                ignore_errors = True
            else:
                ignore_errors = False
        elif arg == "--force":
            if value == 'true' or value == 'True':
                force = True
            else:
                force = False
        elif arg == "--drop-old-data":
            if value == 'true' or value == 'True':
                drop_old_data = True
            else:
                drop_old_data = False
        elif arg == "--do-execute":
            if value == 'true' or value == 'True':
                do_execute = True
            else:
                do_execute = False
        elif arg == "--slot-count":
            query_slot_count = int(value)
        elif arg == "--comprows":
            comprows = int(value)
        elif arg == "--query_group":
            if value != '' and value != None:
                query_group = value
        elif arg == "--ssl-option":
            if value == 'true' or value == 'True':
                ssl_option = True
            else:
                ssl_option = False
        else:
            assert False, "Unsupported Argument " + arg
            usage()
    
    # Validate that we've got all the args needed
    if db == None:
        usage("Missing Parameter 'db'")
    if db_user == None:
        usage("Missing Parameter 'db-user'")
    if db_host == None:        
        usage("Missing Parameter 'db-host'")
    if db_port == None:        
        usage("Missing Parameter 'db-port'")
    if output_file == None:
        usage("Missing Parameter 'output-file'")
    if analyze_schema == None:
        analyze_schema = 'public'
    if target_schema == None:
        target_schema = analyze_schema
        
    # Reduce to 1 thread if we're analyzing a single table
    if analyze_table != None:
        threads = 1
        
    # get the database password
    if not db_pwd:
        db_pwd = getpass.getpass("Password <%s>: " % db_user)
    
    # setup the configuration
    configure(output_file, db, db_user, db_pwd, db_host, db_port, analyze_schema, target_schema, analyze_table, analyze_col_width, threads, do_execute, query_slot_count, ignore_errors, force, drop_old_data, comprows, query_group, debug, ssl_option)
    
    # run the analyser
    result_code = run()
    
    # exit based on the provided return code
    sys.exit(result_code)

if __name__ == "__main__":
    main(sys.argv)

# This file is part of the Hotwire Shell project API.

# Copyright (C) 2007,2008 Colin Walters <walters@verbum.org>

# Permission is hereby granted, free of charge, to any person obtaining a copy 
# of this software and associated documentation files (the "Software"), to deal 
# in the Software without restriction, including without limitation the rights 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies 
# of the Software, and to permit persons to whom the Software is furnished to do so, 
# subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all 
# copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A 
# PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE X CONSORTIUM BE 
# LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, 
# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR 
# THE USE OR OTHER DEALINGS IN THE SOFTWARE.

import os,sys,locale

from hotwire.builtin import builtin_hotwire, InputStreamSchema, MultiArgSpec
from hotwire.command import Pipeline,HotwireContext

@builtin_hotwire(options_passthrough=True,
                 input=InputStreamSchema('any'))
def apply(context, *args):
    _("""Like Unix xargs - take input and convert to arguments.""")

    newargs = list(args)
    for argument in context.input:
        if not isinstance(argument, basestring):
            argument = unicode(argument)
        newargs.append(argument)
        
    new_context = HotwireContext(initcwd=context.cwd)
    # TODO - pull in resolver from shell.py?  Should this function expand
    # aliases?        
    pipeline = Pipeline.create(new_context, None, *newargs)
    pipeline.execute_sync(assert_all_threaded=True)
    for result in pipeline.get_output():
        yield result

#!/usr/bin/env python

'''
analyze-vacuum-schema.py
* Copyright 2015, Amazon.com, Inc. or its affiliates. All Rights Reserved.
*
* Licensed under the Amazon Software License (the "License").
* You may not use this file except in compliance with the License.
* A copy of the License is located at
*
* http://aws.amazon.com/asl/
*
* or in the "license" file accompanying this file. This file is distributed
* on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
* express or implied. See the License for the specific language governing
* permissions and limitations under the License.

The Redshift Analyze Vacuum Utility gives you the ability to automate VACUUM and ANALYZE operations.
When run, it will analyze or vacuum an entire schema or individual tables. This Utility Analyzes
and Vacuums table(s) in a Redshift Database schema, based on certain parameters like unsorted,
stats off and size of the table and system alerts from stl_explain & stl_alert_event_log.
By turning on/off '--analyze-flag' and  '--vacuum-flag' parameters, you can run it as  'vacuum-only'
or  'analyze-only' utility. This script can be scheduled to run VACUUM and ANALYZE as part of
regular maintenance/housekeeping activities, when there are less database activities (quiet period).

This script will:

   1) Analyze a single table or tables in a schema based on,
        a) Alerts from stl_explain & stl_alert_event_log.
        b) 'stats off' metrics from SVV_TABLE_INFO.

   2) Vacuum a single table or tables in a schema based on,
        a) The alerts from stl_alert_event_log.
        b) The 'unsorted' and 'size' metrics from SVV_TABLE_INFO.
        c) Vacuum reindex to analyze the interleaved sort keys

Srinikri Amazon Web Services (2015)

11/21/2015 : Added support for vacuum reindex to analyze the interleaved sort keys.

'''

import sys
import pg
import getopt
import os
import re
import getpass
import traceback
import datetime
from string import uppercase

__version__ = ".9.1.3.4"

ERROR = 1
INVALID_ARGS = 2
NO_WORK = 3
TERMINATED_BY_USER = 4
NO_CONNECTION = 5

# timeout for retries - 100ms
RETRY_TIMEOUT = 100/1000


master_conn = None
db_connections = {}
db = None
db_user = None
db_pwd = None
db_host = None
db_port = 5439
schema_name = 'public'
table_name = None
debug = False
output_file_handle = None
do_execute = False
query_slot_count = 1
ignore_errors = False
query_group = None

#set default values to vacuum, analyze variables

analyze_flag       = True
vacuum_flag        = True
vacuum_parameter   = 'FULL'
min_unsorted_pct   = 05
max_unsorted_pct   = 50
deleted_pct        = 05
stats_off_pct      = 10
max_table_size_mb  = (700*1024)
goback_no_of_days  = 1
query_rank         = 25

def execute_query(str):
    conn = get_pg_conn()
    result = None
    query_result = conn.query(str)

    if query_result is not None:
        result = query_result.getresult()
        query_count = len(result)

        if debug:
            comment('Query Execution returned %s Results' % (len(result)))

    return result

def commit():
    execute_query('commit')

def rollback():
    execute_query('rollback')

def close_conn(conn):
    try:
        conn.close()
    except Exception as e:
        if debug:
            print(e)

def cleanup():
    # close all connections and close the output file
    if master_conn != None:
        close_conn(master_conn)

    for key in db_connections:
        if db_connections[key] != None:
            close_conn(db_connections[key])

    if output_file_handle != None:
        output_file_handle.close()

def comment(string):
    datetime_str = str(datetime.datetime.now())
    if (string != None):
        if re.match('.*\\n.*',string) != None:
            write('/* [%s]\n%s\n*/\n' % (str(os.getpid()),string))
        else:
            write('-- %s [%s] %s' % (datetime_str,str(os.getpid()),string))

def print_statements(statements):
    if statements != None:
        for s in statements:
            if s != None:
                write(s)

def write(s):
    # write output to all the places we want it
    print(s)
    if output_file_handle != None:
        output_file_handle.write( str(s) + "\n")
        output_file_handle.flush()

def get_pg_conn():
    global db_connections
    pid = str(os.getpid())

    conn = None

    # get the database connection for this PID
    try:
        conn = db_connections[pid]
    except KeyError:
        pass

    if conn == None:
        # connect to the database
        if debug:
            comment('Connect [%s] %s:%s:%s:%s' % (pid,db_host,db_port,db,db_user))

        try:
            options = 'keepalives=1 keepalives_idle=200 keepalives_interval=200 keepalives_count=5'
            connection_string = "host=%s port=%s dbname=%s user=%s password=%s %s" % (db_host, db_port, db, db_user, db_pwd, options)

            conn = pg.connect(dbname=connection_string)
        except Exception as e:
            write(e)
            write('Unable to connect to Cluster Endpoint')
            cleanup()
            sys.exit(ERROR)

        # set default search path
        search_path = 'set search_path = \'$user\',public,%s' % (schema_name)

        if debug:
            comment(search_path)

        try:
            conn.query(search_path)
        except pg.ProgrammingError as e:
            if re.match('schema "%s" does not exist' % (schema_name,),e.message) != None:
                write('Schema %s does not exist' % (schema_name,))
            else:
                write(e.message)
            return None

        if query_group is not None:
            set_query_group = 'set query_group to %s' % (query_group)

            if debug:
                comment(set_query_group)

            conn.query(set_query_group)

        if query_slot_count != 1:
            set_slot_count = 'set wlm_query_slot_count = %s' % (query_slot_count)

            if debug:
                comment(set_slot_count)

            conn.query(set_slot_count)

        # set a long statement timeout
        set_timeout = "set statement_timeout = '36000000'"
        if debug:
            comment(set_timeout)

        conn.query(set_timeout)

        # cache the connection
        db_connections[pid] = conn

    return conn


def run_commands(conn, commands):
    for idx,c in enumerate(commands,start=1):
        if c != None:

            comment('[%s] Running %s out of %s commands: %s' % (str(os.getpid()),idx,len(commands),c))
            try:
                conn.query(c)
                comment('Success.')
            except Exception as e:
                # cowardly bail on errors
                rollback()
                write(traceback.format_exc())
                return False

    return True

def run_vacuum(conn):

    statements =[]

    if table_name != None:

        get_vacuum_statement = '''SELECT DISTINCT 'vacuum %s ' + "schema" + '."' + "table" + '" ; '
                                                   + '/* '+ ' Table Name : ' + "schema" + '."' + "table"
                                                   + '",  Size : ' + CAST("size" AS VARCHAR(10)) + ' MB,  Unsorted_pct : ' + CAST("unsorted" AS VARCHAR(10))
                                                   + ',  Deleted_pct : ' + CAST("empty" AS VARCHAR(10)) +' */ ;'

                                        FROM svv_table_info
                                        WHERE (unsorted > %s OR empty > %s)
                                            AND   size < %s
                                            AND  "schema" = '%s'
                                            AND  "table" = '%s';
                                        ''' % (vacuum_parameter,min_unsorted_pct,deleted_pct,max_table_size_mb,schema_name,table_name)
    else:

        # query for all tables in the schema ordered by size descending
        comment("Extracting Candidate Tables for vacuum based on the alerts...")

        get_vacuum_statement = '''
                SELECT DISTINCT 'vacuum %s ' + feedback_tbl.schema_name + '."' + feedback_tbl.table_name + '" ; '
                + '/* '+ ' Table Name : ' + info_tbl."schema" + '."' + info_tbl."table"
                                                   + '",  Size : ' + CAST(info_tbl."size" AS VARCHAR(10)) + ' MB'
                                                   + ',  Unsorted_pct : ' + COALESCE(CAST(info_tbl."unsorted" AS VARCHAR(10)), 'N/A')
                                                   + ',  Deleted_pct : ' + CAST(info_tbl."empty" AS VARCHAR(10)) +' */ ;'
                    FROM (SELECT schema_name,
                                 table_name
                          FROM (SELECT TRIM(n.nspname) schema_name,
                                       c.relname table_name,
                                       DENSE_RANK() OVER (ORDER BY COUNT(*) DESC) AS qry_rnk,
                                       COUNT(*)
                                FROM stl_alert_event_log AS l
                                  JOIN (SELECT query,
                                               tbl,
                                               perm_table_name
                                        FROM stl_scan
                                        WHERE perm_table_name <> 'Internal Worktable'
                                        GROUP BY query,
                                                 tbl,
                                                 perm_table_name) AS s ON s.query = l.query
                                  JOIN pg_class c ON c.oid = s.tbl
                                  JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace
                                WHERE l.userid > 1
                                AND   l.event_time >= dateadd (DAY,-%s,CURRENT_DATE)
                                AND   l.Solution LIKE '%%VACUUM command%%'
                                GROUP BY TRIM(n.nspname),
                                         c.relname) anlyz_tbl
                          WHERE anlyz_tbl.qry_rnk < %s) feedback_tbl
                      JOIN svv_table_info info_tbl
                        ON info_tbl.schema = feedback_tbl.schema_name
                       AND info_tbl.table = feedback_tbl.table_name
                    WHERE /*(info_tbl.unsorted > %s OR info_tbl.empty > %s) AND */
                        info_tbl.size < %s
                        AND   TRIM(info_tbl.schema) = '%s'
                        AND   (sortkey1 not ilike  'INTERLEAVED%%' OR sortkey1 IS NULL)
                    ORDER BY info_tbl.size ASC, info_tbl.skew_rows ASC;
                            ''' %(vacuum_parameter,goback_no_of_days,query_rank,min_unsorted_pct,deleted_pct,max_table_size_mb,schema_name,)

    if debug:
        comment(get_vacuum_statement)

    vacuum_statements = execute_query(get_vacuum_statement)

    for vs in vacuum_statements:
        statements.append(vs[0])

    if not run_commands(conn, statements):
                    if not ignore_errors:
                        if debug:
                            write("Error running statements: %s" % (str(statements),))
                        return ERROR

    statements =[]
    if table_name == None:

        # query for all tables in the schema ordered by size descending
        comment("Extracting Candidate Tables for vacuum ...")
        get_vacuum_statement = '''SELECT DISTINCT 'vacuum %s ' + "schema" + '."' + "table" + '" ; '
                                                   + '/* '+ ' Table Name : ' + "schema" + '."' + "table"
                                                   + '",  Size : ' + CAST("size" AS VARCHAR(10)) + ' MB'
                                                   + ',  Unsorted_pct : ' + COALESCE(CAST(info_tbl."unsorted" AS VARCHAR(10)), 'N/A')
                                                   + ',  Deleted_pct : ' + CAST("empty" AS VARCHAR(10)) +' */ ;'

                                        FROM svv_table_info info_tbl
                                        WHERE "schema" = '%s'
                                                AND
                                                 (
                                                --If the size of the table is less than the max_table_size_mb then , run vacuum based on condition: >min_unsorted_pct AND >deleted_pct
                                                    ((size < %s) AND (unsorted > %s OR empty > %s))
                                                    OR
                                                --If the size of the table is greater than the max_table_size_mb then , run vacuum based on condition:
                                                -- >min_unsorted_pct AND < max_unsorted_pct AND >deleted_pct
                                                --This is to avoid big table with large unsorted_pct
                                                     ((size > %s) AND (unsorted > %s AND unsorted < %s ))
                                                 )
                                                AND (sortkey1 not ilike  'INTERLEAVED%%' OR sortkey1 IS NULL)
                                        ORDER BY "size" ASC ,skew_rows ASC;

                                        ''' %(vacuum_parameter,schema_name,max_table_size_mb,min_unsorted_pct,
                                              deleted_pct,max_table_size_mb,min_unsorted_pct,max_unsorted_pct)

        if debug:
            comment(get_vacuum_statement)

        vacuum_statements = execute_query(get_vacuum_statement)

        for vs in vacuum_statements:
            statements.append(vs[0])

        if not run_commands(conn, statements):
            if not ignore_errors:
                if debug:
                    write("Error running statements: %s" % (str(statements),))
                return ERROR

    statements =[]
    if table_name == None:

        # query for all tables in the schema for vacuum reindex

        comment("Extracting Candidate Tables for vacuum reindex ...")
        get_vacuum_statement = ''' SELECT DISTINCT 'vacuum REINDEX ' + schema_name + '."' + table_name + '" ; ' + '/* ' + ' Table Name : '
                                    + schema_name + '."' + table_name + '",  Rows : ' + CAST("rows" AS VARCHAR(10))
                                    + ',  Interleaved_skew : ' + CAST("max_skew" AS VARCHAR(10))
                                    + ' ,  Reindex Flag : '  + CAST(reindex_flag AS VARCHAR(10)) + ' */ ;'

                                FROM (SELECT TRIM(n.nspname) schema_name, t.relname table_name,
                                                 MAX(v.interleaved_skew) max_skew, MAX(c.count) AS rows,
                                                 CASE
                                                   WHEN (max(c.max_bucket) = 0) OR (MAX(v.interleaved_skew) > 5 AND MAX(c.count) > 10240) THEN 'Yes'
                                                   ELSE 'No'
                                                 END AS reindex_flag
                                            FROM svv_interleaved_columns v
                                            JOIN (SELECT tbl,col, max(compressed_val) AS max_bucket,  SUM(count) AS count
                                                  FROM stv_interleaved_counts
                                                  GROUP BY tbl,col) c
                                            ON (v.tbl = c.tbl AND v.col = c.col)
                                            JOIN pg_class t ON t.oid = c.tbl
                                            JOIN pg_catalog.pg_namespace n ON n.oid = t.relnamespace
                                            GROUP BY 1, 2)
                                WHERE reindex_flag = 'Yes'
                                    AND schema_name = '%s'
                                        ''' %(schema_name)

        if debug:
            comment(get_vacuum_statement)

        vacuum_statements = execute_query(get_vacuum_statement)

        for vs in vacuum_statements:
            statements.append(vs[0])

        if not run_commands(conn, statements):
            if not ignore_errors:
                if debug:
                    write("Error running statements: %s" % (str(statements),))
                return ERROR

    return True

def run_analyze(conn):

    statements =[]

    if table_name != None:

        # If it is one table , just check if this needs to be analyzed and prepare analyze statements

        get_analyze_statement_feedback = '''SELECT DISTINCT 'analyze ' + "schema" + '."' + "table" + '" ; '
                                                   + '/* '+ ' Table Name : ' + "schema" + '."' + "table"
                                                   + '",  stats_off : ' + CAST("stats_off" AS VARCHAR(10)) + ' */ ;'
                                                FROM svv_table_info
                                                WHERE   stats_off::DECIMAL (32,4) > %s ::DECIMAL (32,4)
                                                AND  trim("schema") = '%s'
                                                AND  trim("table") = '%s';
                                                ''' % (stats_off_pct,schema_name,table_name,)
    else:

        # query for all tables in the schema
        comment("Extracting Candidate Tables for analyze based on Query Optimizer Alerts(Feedbacks) ...")

        get_analyze_statement_feedback = '''
                                 --Get top N rank tables based on the missing statistics alerts

                                    SELECT DISTINCT 'analyze ' + feedback_tbl.schema_name + '."' + feedback_tbl.table_name + '" ; '
                                    + '/* '+ ' Table Name : ' + info_tbl."schema" + '."' + info_tbl."table"
                                        + '", Stats_Off : ' + CAST(info_tbl."stats_off" AS VARCHAR(10)) + ' */ ;'
                                    FROM ((SELECT TRIM(n.nspname) schema_name,
                                          c.relname table_name
                                   FROM (SELECT TRIM(SPLIT_PART(SPLIT_PART(a.plannode,':',2),' ',2)) AS Table_Name,
                                                COUNT(a.query),
                                                DENSE_RANK() OVER (ORDER BY COUNT(a.query) DESC) AS qry_rnk
                                         FROM stl_explain a,
                                              stl_query b
                                         WHERE a.query = b.query
                                         AND   CAST(b.starttime AS DATE) >= dateadd (DAY,-%s,CURRENT_DATE)
                                         AND   a.userid > 1
                                         AND   a.plannode LIKE '%%missing statistics%%'
                                         AND   a.plannode NOT LIKE '%%_bkp_%%'
                                         GROUP BY Table_Name) miss_tbl
                                     LEFT JOIN pg_class c ON c.relname = TRIM (miss_tbl.table_name)
                                     LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace
                                   WHERE miss_tbl.qry_rnk <= %s)

                                   -- Get the top N rank tables based on the stl_alert_event_log alerts

                                   UNION
                                   SELECT schema_name,
                                          table_name
                                   FROM (SELECT TRIM(n.nspname) schema_name,
                                                c.relname table_name,
                                                DENSE_RANK() OVER (ORDER BY COUNT(*) DESC) AS qry_rnk,
                                                COUNT(*)
                                         FROM stl_alert_event_log AS l
                                           JOIN (SELECT query,
                                                        tbl,
                                                        perm_table_name
                                                 FROM stl_scan
                                                 WHERE perm_table_name <> 'Internal Worktable'
                                                 GROUP BY query,
                                                          tbl,
                                                          perm_table_name) AS s ON s.query = l.query
                                           JOIN pg_class c ON c.oid = s.tbl
                                           JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace
                                         WHERE l.userid > 1
                                         AND   l.event_time >= dateadd (DAY,-%s,CURRENT_DATE)
                                         AND   l.Solution LIKE '%%ANALYZE command%%'
                                         GROUP BY TRIM(n.nspname),
                                                  c.relname) anlyz_tbl
                                   WHERE anlyz_tbl.qry_rnk < %s) feedback_tbl
                              JOIN svv_table_info info_tbl
                                ON info_tbl.schema = feedback_tbl.schema_name
                               AND info_tbl.table = feedback_tbl.table_name
                            WHERE info_tbl.stats_off::DECIMAL (32,4) > %s::DECIMAL (32,4)
                            AND   TRIM(info_tbl.schema) = '%s'
                            ORDER BY info_tbl.size ASC  ;
                            ''' % (goback_no_of_days,query_rank,goback_no_of_days,query_rank,stats_off_pct,schema_name)

        #print(get_analyze_statement_feedback)
    if debug:
        comment(get_analyze_statement_feedback)

    analyze_statements = execute_query(get_analyze_statement_feedback)

    for vs in analyze_statements:
        statements.append(vs[0])

    if not run_commands(conn, statements):
                    if not ignore_errors:
                        if debug:
                            write("Error running statements: %s" % (str(statements),))
                        return ERROR

    if table_name == None:

        comment("Extracting Candidate Tables for analyze based on stats off from system table info ...")

        get_analyze_statement = '''SELECT DISTINCT 'analyze ' + "schema" + '."' + "table" + '" ; '
                                        + '/* '+ ' Table Name : ' + "schema" + '."' + "table"
                                        + '", Stats_Off : ' + CAST("stats_off" AS VARCHAR(10)) + ' */ ;'
                                        FROM svv_table_info
                                        WHERE   stats_off::DECIMAL (32,4) > %s::DECIMAL (32,4)
                                        AND  trim("schema") = '%s'
                                        ORDER BY "size" ASC ;
                                        ''' % (stats_off_pct,schema_name)

        if debug:
            comment(get_analyze_statement)

        analyze_statements = execute_query(get_analyze_statement)

        statements =[]
        for vs in analyze_statements:
            statements.append(vs[0])

        if not run_commands(conn, statements):
                if not ignore_errors:
                    if debug:
                        write("Error running statements: %s" % (str(statements),))
                        return ERROR
    return True

def usage(with_message):
    write('Usage: analyze-vacuum-schema.py')
    write('       Runs vacuum AND/OR analyze on table(s) in a schema\n')

    if with_message != None:
        write(with_message + "\n")

    write('Arguments: --db                 - The Database to Use')
    write('           --db-user            - The Database User to connect to')
    write('           --db-pwd             - The Password for the Database User to connect to')
    write('           --db-host            - The Cluster endpoint')
    write('           --db-port            - The Cluster endpoint port : Default = 5439')
    write('           --schema-name        - The Schema to be Analyzed or Vacuumed : Default = public')
    write('           --table-name         - A specific table to be Analyzed or Vacuumed, if --analyze-schema is not desired')
    write('           --output-file        - The full path to the output file to be generated')
    write('           --debug              - Generate Debug Output including SQL Statements being run')
    write('           --slot-count         - Modify the wlm_query_slot_count : Default = 1')
    write('           --ignore-errors      - Ignore errors raised when running and continue processing')
    write('           --query_group        - Set the query_group for all queries')
    write('           --analyze-flag       - Flag to turn ON/OFF ANALYZE functionality (True or False) : Default = True ' )
    write('           --vacuum-flag        - Flag to turn ON/OFF VACUUM functionality (True or False) :  Default = True')
    write('           --vacuum-parameter   - Vacuum parameters [ FULL | SORT ONLY | DELETE ONLY | REINDEX ] Default = FULL')
    write('           --min-unsorted-pct   - Minimum unsorted percentage(%) to consider a table for vacuum : Default = 05%')
    write('           --max-unsorted-pct   - Maximum unsorted percentage(%) to consider a table for vacuum : Default = 50%')
    write('           --deleted-pct        - Minimum deleted percentage (%) to consider a table for vacuum: Default = 05%')
    write('           --stats-off-pct      - Minimum stats off percentage(%) to consider a table for analyze : Default = 10%')
    write('           --max-table-size-mb  - Maximum table size in MB : Default = 700*1024 MB')

    sys.exit(INVALID_ARGS)


def main(argv):
    supported_args = """db= db-user= db-pwd= db-host= db-port= schema-name= table-name= debug= output-file= slot-count= ignore-errors= query_group= analyze-flag= vacuum-flag= vacuum-parameter= min-unsorted-pct= max-unsorted-pct= deleted-pct= stats-off-pct= max-table-size-mb="""

    # extract the command line arguments
    try:
        optlist, remaining = getopt.getopt(argv[1:], "", supported_args.split())
    except getopt.GetoptError as err:
        print str(err)
        usage(None)

    # setup globals
    global master_conn
    global db
    global db_user
    global db_pwd
    global db_host
    global db_port
    global schema_name
    global table_name
    global debug
    global output_file_handle
    global query_slot_count
    global ignore_errors
    global query_group
    global analyze_flag
    global vacuum_flag
    global vacuum_parameter
    global min_unsorted_pct
    global max_unsorted_pct
    global deleted_pct
    global stats_off_pct
    global max_table_size_mb


    output_file = None

    # parse command line arguments
    for arg, value in optlist:
        if arg == "--db":
            if value == '' or value == None:
                usage()
            else:
                db = value
        elif arg == "--db-user":
            if value == '' or value == None:
                usage()
            else:
                db_user = value
        elif arg == "--db-pwd":
            if value == '' or value == None:
                usage()
            else:
                db_pwd = value
        elif arg == "--db-host":
            if value == '' or value == None:
                usage()
            else:
                db_host = value
        elif arg == "--db-port":
            if value != '' and value != None:
                db_port = value
        elif arg == "--schema-name":
            if value != '' and value != None:
                schema_name = value
        elif arg == "--table-name":
            if value != '' and value != None:
                table_name = value
        elif arg == "--debug":
            if value.upper() == 'TRUE':
                debug = True
            else:
                debug = False
        elif arg == "--output-file":
            if value == '' or value == None:
                usage()
            else:
                output_file = value
        elif arg == "--ignore-errors":
            if value.upper() == 'TRUE':
                ignore_errors = True
            else:
                ignore_errors = False
        elif arg == "--slot-count":
            query_slot_count = int(value)
        elif arg == "--query_group":
            if value != '' and value != None:
                query_group = value
        elif arg == "--vacuum-flag":
            if value.upper() == 'FALSE':
                vacuum_flag = False
        elif arg == "--analyze-flag":
            if value.upper()  == 'FALSE':
                analyze_flag = False
        elif arg == "--vacuum-parameter":
            if value.upper() == 'SORT ONLY' or value.upper() == 'DELETE ONLY' or value.upper() == 'REINDEX' :
                vacuum_parameter = value
            else:
                vacuum_parameter = 'FULL'
        elif arg == "--min-unsorted-pct":
            if value != '' and value != None:
                min_unsorted_pct = value
        elif arg == "--max-unsorted-pct":
            if value != '' and value != None:
                max_unsorted_pct = value
        elif arg == "--deleted-pct":
            if value != '' and value != None:
                deleted_pct = value
        elif arg == "--stats-off-pct":
            if value != '' and value != None:
                stats_off_pct = value
        elif arg == "--max-table-size-mb":
            if value != '' and value != None:
                max_table_size_mb = value
        else:
            assert False, "Unsupported Argument " + arg
            usage()

    # Validate that we've got all the args needed
    if db == None:
        usage("Missing Parameter 'db'")
    if db_user == None:
        usage("Missing Parameter 'db-user'")
    if db_pwd == None:
        usage("Missing Parameter 'db-pwd'")
    if db_host == None:
        usage("Missing Parameter 'db-host'")
    if db_port == None:
        usage("Missing Parameter 'db-port'")
    if output_file == None:
        usage("Missing Parameter 'output-file'")


    # get the database password
    #db_pwd = getpass.getpass("Password <%s>: " % db_user)

    # open the output file
    output_file_handle = open(output_file,'w')

    # get a connection for the controlling processes
    master_conn = get_pg_conn()

    if master_conn == None:
        sys.exit(NO_CONNECTION)

    comment("Connected to %s:%s:%s as %s" % (db_host, db_port, db, db_user))

    if vacuum_flag != False:
        # Run vacuum based on the Unsorted , Stats off and Size of the table
        run_vacuum(master_conn)
    else:
        comment("vacuum flag arg is set as %s.Vacuum is not performed." % (vacuum_flag))

    if analyze_flag != False:
        # Run Analyze based on the  Stats off Metrics table
        run_analyze(master_conn)
    else:
        comment("anlayze flag arg is set as %s.Analyze is not performed." % (analyze_flag))

    comment('Processing Complete')
    cleanup()

if __name__ == "__main__":
    main(sys.argv)

from django.apps import AppConfig


class ProfilesConfig(AppConfig):
    name = 'profiles'
